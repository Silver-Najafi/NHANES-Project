---
title: "NHANES 2013–2014 cycle Project Report"
author:  |
  Noghre Najafi  
  *Supervised by Dr. Mark Ghamsary, Professor (Retired), UCLA*
date: "`r format(Sys.Date(), '%Y-%m-%d')`"
output: 
  html_document:
    theme: cerulean
    highlight: tango
    toc: true
    toc_depth: 3
    toc_float: true
    number_sections: true
bibliography: "D:/OIST/NLR/references.bib"
csl: "D:/OIST/NLR/apa.csl"


---


# **Introduction**

## **Data Source**

Data were obtained from the **NHANES 2013–2014 cycle**, a nationally representative cross-sectional survey of the U.S. population. This cycle includes approximately **10,000 participants**, providing demographic, laboratory, physical examination, and questionnaire data collected under standardized protocols.

## **Variables**

- **Outcome variable:** hsCRP (high-sensitivity C-reactive protein)

- **Clinical/laboratory:** SBP, DBP, Cholesterol, LDL, HDL, Triglycerides, Glucose, BMI, and complete blood count indices (WBC, HGB, RBC, MCV, MCH, MCHC, RDW, MPV, PLT).

- **Trace elements:** Trace elements: Copper, Zinc (available for a subsample of ~2,500–3,000 participants)

- **Derived variables:** LDL/HDL ratio, Zinc/Copper ratio

- **Health history:** CVD (0/1), Diabetes, Hypertension, History of CVD, History of Diabetes.

- **Psychosocial/lifestyle:** Depression score (PHQ-9), Physical Activity Level (PAL), Age, Sex (male = 0, female = 1).

## **Notes**

- Analyses focused on hsCRP (outcome) with **HDL, DBP, Cholesterol, BMI, Age, Sex, CVD, and PAL** as predictors.

- NHANES data are collected under rigorous quality control in clinical laboratories.

- Sample sizes varied by variable due to subsampling (e.g., trace elements) and missing data.

# **Project Goal**

The objective of this project is to **implement and evaluate three regression models Linear Regression, Binary Logistic Regression, and Ordinal Logistic Regression** to predict **hsCRP** using NHANES 2013–2014 data.

This nationally representative dataset provides a strong foundation for assessing model performance on a real-world public health problem. The analysis aims to:

1- Compare model performance and identify the most accurate approach.

2- Determine key predictors associated with hsCRP.

3- Highlight the potential of statistical modeling to inform clinical and public health decision making.

# **Data Cleaning**

## **missing data**

First, we look for missing data to find out how many there are:


```{r, echo=FALSE, message=FALSE, warning=FALSE}

library(haven)
data <- read_sav("D:/OIST/OLR/NHANES_data.sav")
# Count number of missing values per selected variable
vars <- c("hsCRP", "PAL", "Age", "Sex", 
          "DBP", "Cholesterol", "BMI","CVD" , "HDL")

## Missing Data Summary

missing_summary <- sapply(data[vars], function(x) sum(is.na(x)))
missing_table <- data.frame(
  Variable = names(missing_summary),
  Missing_Count = missing_summary,
  row.names = NULL
)

knitr::kable(missing_table, caption = "Missing Data Summary")
             
```

Approximately **10% of the dataset contained missing values**, representing a non-negligible portion that could bias results if ignored.

### **Multiple imputation with MCMC**

- Missing values were imputed using the **Markov Chain Monte Carlo (MCMC)** method implemented in the `mice` package in R.

- Convergence was assessed by examining trace plots of the **mean and standard deviation** of imputed values across iterations.

- The trace plots were stable, indicating that the **imputation chains converged successfully**.

- This approach ensures that uncertainty due to missing data is appropriately incorporated into subsequent analyses.

```{r echo=FALSE, message=FALSE, warning=FALSE }

set.seed(123)
library(dplyr)
library(mice)
library(haven)  # for as_factor()

# Select the variables of interest
vars <- c("hsCRP", "PAL", "Age", "DBP", 
          "Cholesterol", "BMI", "HDL", "Sex","CVD")

data_subset <- data %>%
  select(all_of(vars))

# Convert labelled data to plain numeric/factor before any processing
data_subset <- data_subset %>%
  mutate(across(everything(), ~ {
    if (inherits(.x, "haven_labelled")) {
      as.numeric(.x)
    } else {
      .x
    }
  }))

# Continuous variables to numeric
continuous_vars <- c("hsCRP", "PAL", "Age", 
                     "DBP", "Cholesterol", "BMI", "HDL")

data_subset[continuous_vars] <- lapply(
  data_subset[continuous_vars],
  function(x) as.numeric(as.character(x))
)

# Binary variables to factor
binary_vars <- c("CVD", "Sex")

data_subset[binary_vars] <- lapply(
  data_subset[binary_vars],
  function(x) factor(as.numeric(as.character(x)), levels = c(0,1))
)

# Create method vector
meth <- make.method(data_subset)

# Assign imputation methods
meth[continuous_vars] <- "norm"
meth[binary_vars] <- "logreg"

# Run MICE
imp <- mice(data_subset, method = meth, m =10, seed = 123)

clean_data <- complete(imp, 1)
# Summary and diagnostics
plot(imp)

```

- The **left panel** of the trace plots displays the mean of imputed values across iterations for each dataset (colored lines). The lines remain relatively flat and stable, indicating convergence of the means.
- The **right panel** shows the standard deviations of the imputed values. These lines are also stable across iterations, confirming convergence in terms of variability.

- Taken together, the plots demonstrate that the **MCMC imputation process achieved convergence**, with no evidence of systematic drift or major fluctuations.

### **Distribution of imputed values**

```{r, echo=FALSE, message=FALSE, warning=FALSE}

library(mice)
# for continuous 
densityplot(imp, ~ hsCRP + PAL + Age + DBP + Cholesterol + BMI + HDL)

```

- The distributions of imputed values were examined to assess stability and convergence.

- **PAL, Age, DBP, BMI, HDL, and Cholesterol** exhibited relatively stable distributions across imputations, indicating good convergence.

- **hsCRP**, in contrast, showed slightly more variability across imputations, suggesting that this variable may be more sensitive to missing data.

- Overall, the imputed values demonstrate **general stability and convergence**, supporting the reliability of subsequent analyses

```{r, echo=FALSE, message=FALSE, warning=FALSE}

library(mice)
# For binaries (stripplot works better)
stripplot(imp, hsCRP + PAL + Age + DBP + Cholesterol + BMI + HDL ~ .imp)

stripplot(imp, CVD + Sex  ~ .imp, pch = 20, cex = 1.2)

```

- The graphs above show the dispersion of the imputed values. We see approximately reasonable dispersions indicating a stable imputation.

## **outliers**

After "imputation" the missing data, we will start discovering the outliers:

```{r, echo=FALSE, message=FALSE, warning=FALSE}

set.seed(123)
options(digits = 6)
library(dplyr)

# Continuous variables
continuous_vars <- c("hsCRP", "PAL",  "Age", 
                     "DBP", "Cholesterol", "BMI", "HDL")


min_value <- min(clean_data$hsCRP, na.rm = TRUE)
max_value <- max(clean_data$hsCRP, na.rm = TRUE)

# Function to detect outlier indices
find_outliers <- function(x) {
  Q1 <- quantile(x, 0.25, na.rm = TRUE)
  Q3 <- quantile(x, 0.75, na.rm = TRUE)
  IQR_val <- Q3 - Q1
  lower_bound <- Q1 - 1.5 * IQR_val
  upper_bound <- Q3 + 1.5 * IQR_val
  which(x < lower_bound | x > upper_bound)
}

# Detect and summarize
outlier_summary_clean <- lapply(continuous_vars, function(var) {
  idx <- find_outliers(clean_data[[var]])
  data.frame(
    Variable = var,
    Num_Outliers = length(idx),
    Percent_Outliers = round(100 * length(idx) / sum(!is.na(clean_data[[var]])), 2)
  )
}) %>% bind_rows()

# Show summary table
outlier_summary_clean
```

### **Step 1 — Extract outliers with ID for review**

- Numerous **outliers** were present in the dataset and could not be ignored.

- Each outlier was linked to its corresponding **participant ID** to review whether the unusual value was isolated or reflected multiple abnormal entries for that individual.

```{r, echo=FALSE, message=FALSE, warning=FALSE , results='hide'}
set.seed(123)
options(digits = 6)
library(dplyr)

# Continuous variables
continuous_vars <- c("hsCRP", "PAL", "Age", "DBP", "Cholesterol", "BMI", "HDL")

# Test each variable individually to find which one causes the error
for (var in continuous_vars) {
  cat("Testing variable:", var, "\n")
  idx <- find_outliers(clean_data[[var]])
  cat("Number of outliers:", length(idx), "\n")
  cat("Length of ID vector:", length(clean_data$ID[idx]), "\n")
  cat("Length of value vector:", length(clean_data[[var]][idx]), "\n")
  cat("---\n")
}
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# Use row numbers as IDs to ensure proper indexing
outlier_details <- lapply(continuous_vars, function(var) {
  idx <- find_outliers(clean_data[[var]])
  if (length(idx) > 0) {
    data.frame(
      Variable = var,
      Row_Number = idx,  # Use actual row numbers
      Value = clean_data[[var]][idx],
      stringsAsFactors = FALSE
    )
  } else {
    NULL
  }
}) %>% bind_rows()

# Show first few results
head(outlier_details, 10)
```

### **Step 2 — Define scientific/logical ranges**

- For each variable, reasonable scientific or logical ranges were established to capture plausible values while minimizing undue dispersion:

 * **hsCRP:** 0-8 mg/L
 * **PAL (physical activity index):** 1.2-2.5 
 * **Age:** 27-73 years
 * **DBP:** 40-130 mmHg
 * **Cholesterol:** 70-200 mg/dL
 * **BMI:** 16-40 kg/m²
 * **HDL:** 30-120 mg/dL

### **Step 3 — Fix outliers**

- Values falling outside the defined ranges were adjusted or removed according to the specified thresholds.

- This approach ensures that **extreme or erroneous entries do not bias subsequent analyses**.

```{r, echo=FALSE, message=FALSE, warning=FALSE }

##########Step 3 — Fix outliers (remove or winsorize)########
library(dplyr)

#  winsorization
winsorize_var <- function(x, lower, upper) {
  x <- ifelse(x < lower, lower, x)
  x <- ifelse(x > upper, upper, x)
  return(x)
}

clean_data <- clean_data %>%
  mutate(
    hsCRP = winsorize_var(hsCRP, 0, 8),
    PAL = winsorize_var(PAL, 1.2, 2.5),
    Age = winsorize_var(Age, 27, 73),
    DBP = winsorize_var(DBP, 40, 130),
    Cholesterol = winsorize_var(Cholesterol, 70, 200),
    BMI = winsorize_var(BMI, 16, 40),
    HDL = winsorize_var(HDL, 30, 120)
  )
 min(clean_data$hsCRP, na.rm = TRUE)
 max(clean_data$hsCRP, na.rm = TRUE)

# min and max
winsor_summary <- data.frame(
  Variable = continuous_vars,
  Min_After_Winsor = sapply(continuous_vars, function(var) min(clean_data[[var]], na.rm = TRUE)),
  Max_After_Winsor = sapply(continuous_vars, function(var) max(clean_data[[var]], na.rm = TRUE)),
  row.names = NULL
)

# dislay table
print(winsor_summary)
hist(clean_data$hsCRP , col = "pink", border = "white")


```

# **Model Building (Linear Regression)**

A **Linear Regression** model was first implemented to provide a baseline prediction of hsCRP.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# Load necessary packages
library(broom)
library(knitr)
library(formattable)
library(car)        # For VIF calculation
library(broom)      # For tidy model summaries
library(dplyr)
# Fit the linear regression model

lm_model <- lm(hsCRP ~ PAL + Age + DBP +Cholesterol + BMI + HDL + CVD + Sex ,data= clean_data)

print(lm_model$call)
# Create coefficient table using broom
model_summary <- tidy(lm_model)
model_glance <- glance(lm_model)
# coefficient 
cat(" coefficients table \n")
kable(model_summary, digits = 4, caption = "coefficients table")

cat("\n Model Performance Metrics\n")
kable(model_glance, digits = 4, caption = "Model Performance Metrics")


library(kableExtra)

# Calculate VIF
vif_values <- vif(lm_model)

# Force into clean data.frame
vif_table <- data.frame(
  Variable = names(vif_values),
  VIF = as.numeric(vif_values)
)

# Nicely formatted table
vif_table %>%
  kbl(caption = "Variance Inflation Factors (VIF)") %>%
  kable_styling(full_width = FALSE, bootstrap_options = c("striped", "hover"))

```

- The model yielded a **low R-squared**, indicating that it explains only a small portion of the variability in hsCRP.

- Despite the low explanatory power, **no multicollinearity** was detected among predictors, suggesting that the estimated coefficients are **stable and interpretable**.

- This baseline model provides a reference point for comparing the performance of more complex regression approaches.

## **Residual Diagnostics**

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(ggplot2)

# 1. Residuals vs Fitted plot (to check homoscedasticity & linearity)
plot(lm_model, which = 1)

# 2. Q-Q plot of residuals (to check normality visually)
plot(lm_model, which = 2)
# 3. Histogram of residuals
ggplot(data.frame(resid = residuals(lm_model)), aes(x = resid)) +
  geom_histogram(color = "black", fill = "lightgreen", bins = 30) +
  theme_minimal() +
  labs(title = "Histogram of Residuals",
       x = "Residuals", y = "Frequency")
```

- The **residuals vs. fitted values plot** indicates that residuals are **not randomly scattered around zero**, suggesting **heteroscedasticity**.

- A distinct linear cluster in the upper portion of the plot may reflect a **subgroup of observations** (e.g., binary predictors such as **CVD and Sex**) that the model does not adequately capture.

- These observations imply that the **assumptions of linear regression are only partially satisfied**.

- To address this issue, further steps may include:

 - **Response variable transformation** (e.g., log(hsCRP), 1/hsCRP)

 - Considering **model refinements** or alternative regression techniques to improve model fit and predictive accuracy.


## **Model Comparison (Linear Regression)**

Now we apply the desired transformations to the response variable to choose the best transformation:

```{r, echo=FALSE, message=FALSE, warning=FALSE}
set.seed(123)
options(digits = 4)

if(!exists("clean_data")) {
  clean_data <- complete(imp, 1)
}
clean_data <- complete(imp, 1)
clean_data$log_hsCRP <- log(clean_data$hsCRP+1)
clean_data$inv_hsCRP <- 1 / (clean_data$hsCRP + 1)

# Libraries
library(MASS)    # For robust regression (rlm)
library(e1071)   # For skewness
library(lmtest)  # For statistical tests


# 1) Original model
cat("\n=== Linear Model ===\n")
lm_model <- lm(hsCRP ~ PAL + Age + DBP + Cholesterol + BMI + HDL + CVD + Sex,
               data = clean_data)
print(lm_model$call)
# 2) Log-transformed model
cat("\n=== Log-transformed Model ===\n")
lm_model_log <- lm(log_hsCRP ~ PAL + Age + DBP + Cholesterol + BMI + HDL + CVD + Sex,
                   data = clean_data)
print(lm_model_log$call)
# 3) Inverse-transformed model
cat("\n=== Inverse-transformed Model ===\n")
lm_model_inv <- lm(inv_hsCRP ~ PAL + Age + DBP + Cholesterol + BMI + HDL + CVD + Sex,
                   data = clean_data)
print(lm_model_inv$call)

# 2) Fit the robust regression model
cat("\n=== Robust Model ===\n")
robust_model <- rlm(hsCRP ~ PAL + Age + DBP + Cholesterol + BMI + HDL + CVD + Sex , data = clean_data)

print(robust_model$call)
# Function to calculate R-squared for rlm
library(MASS)

# Function to calculate pseudo-R² for robust regression
pseudo_r2_rlm <- function(model, y) {
  y_hat <- fitted(model)
  ss_res <- sum((y - y_hat)^2)
  ss_tot <- sum((y - mean(y))^2)
  r2 <- 1 - ss_res/ss_tot
  return(r2)
}

# table for R-squares
r2_results <- data.frame(
  Model = c("Linear", "Log-Transformed", "Inverse-Transformed", "Robust"),
  R2_Type = c("Adjusted R²", "Adjusted R²", "Adjusted R²", "Pseudo-R²"),
  R2_Value = c(
    summary(lm_model)$adj.r.squared,
    summary(lm_model_log)$adj.r.squared,
    summary(lm_model_inv)$adj.r.squared,
    pseudo_r2_rlm(robust_model, clean_data$hsCRP)
  )
)

print(r2_results)

# 5) Optional: Residual plots for visual check
# Set plotting area (2x2 grid)
par(mfrow = c(2, 2))

# 1) Linear regression residuals
hist(residuals(lm_model),
     main = "Residuals: Linear Model",
     xlab = "Residuals",
     col = "lightblue", border = "white")

# 2) Log-transformed linear regression residuals
hist(residuals(lm_model_log),
     main = "Residuals: Log Model",
     xlab = "Residuals",
     col = "lightgreen", border = "white")

# 3) Inverse-transformed linear regression residuals
hist(residuals(lm_model_inv),
     main = "Residuals: Inverse Model",
     xlab = "Residuals",
     col = "lightpink", border = "white")

# 4) Robust regression residuals
hist(residuals(robust_model),
     main = "Residuals: Robust Model",
     xlab = "Residuals",
     col = "yellow", border = "white")


# 6) Cook's distance

# Function to extract max Cook's distance (only for lm models)
get_cooks <- function(model) {
  if("lm" %in% class(model)) {
    return(max(cooks.distance(model)))
  } else {
    return(NA) # not applicable for robust regression
  }
}

# table cook's D
cooks_results <- data.frame(
  Model = c("Linear", "Log-Transformed", "Inverse-Transformed", "Robust"),
  Max_Cooks_D = c(
    get_cooks(lm_model),
    get_cooks(lm_model_log),
    get_cooks(lm_model_inv),
    NA 
  )
)

print(cooks_results)


# 7) Influential points
# Threshold  4/n 
n <- nrow(clean_data)
thr <- 4/n

# Cook's distance for lm models
cD_linear <- cooks.distance(lm_model)
cD_log    <- cooks.distance(lm_model_log)
cD_inv    <- cooks.distance(lm_model_inv)

# Find influential points
inf_points_linear <- which(cD_linear > thr)
inf_points_log    <- which(cD_log > thr)
inf_points_inv    <- which(cD_inv > thr)

# Robust regression - approximate leverage (NOT official Cook's D)

resid_robust <- residuals(robust_model)
leverage_robust <- hatvalues(lm_model) 
cD_robust <- resid_robust^2 * leverage_robust / 
             (robust_model$s^2 * (1 - leverage_robust)^2)

inf_points_robust <- which(cD_robust > thr)

# Print results
cat("Number of influential points (Linear model):", length(inf_points_linear), "\n")
cat("Number of influential points (Log model):", length(inf_points_log), "\n")
cat("Number of influential points (Inverse model):", length(inf_points_inv), "\n")
cat("Number of influential points (Robust model):", length(inf_points_robust), "\n")

```

- Among the tested models, the **log-transformed linear regression** demonstrated the **best performance**, achieving the **highest adjusted R²** and the **fewest influential points**.

- The **inverse-transformed model** performed slightly better than the untransformed linear model but remained **less optimal than the log-transformed model**.

- The **robust regression model**, while less sensitive to outliers, exhibited **weaker explanatory power** based on its pseudo-R² in this dataset.

- Overall, all R² values were relatively low (<10%), suggesting that the current set of predictors may **not fully capture the variability in hsCRP**.

 - This low R² is **not unexpected** given the complex nature of hsCRP and the large, heterogeneous dataset.

 - Additional or alternative predictors may be required to improve model fit.

- **Next steps:** apply **backward elimination** to identify statistically significant predictors and determine the **final parsimonious model**:

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(broom)
library(knitr)
library(formattable)
options(digits = 4)

# Fit models
cat("\n=== Log-transformed Model ===\n")
lm_model_log <- lm(log_hsCRP ~ PAL + DBP + Age +
                 Cholesterol + BMI + HDL + CVD + Sex ,
               data = clean_data)
model_summary0 <- tidy(lm_model_log)
model_glance0 <- glance(lm_model_log)
# coefficient 
cat(" coefficients table  (Log-transformed Model)\n")
kable(model_summary0, digits = 4, caption = "coefficients table")

cat("\n Model Performance Metrics\n")
kable(model_glance0, digits = 4, caption = "Model Performance Metrics")
###

cat("\n=== No-DBP Model ===\n")
lm_model1 <- lm(log_hsCRP ~ PAL + Age +
                        Cholesterol + BMI + HDL + CVD + Sex ,
                      data = clean_data)

model_summary1 <- tidy(lm_model1)
model_glance1 <- glance(lm_model1)
# coefficient 
cat(" coefficients table  (No-DBP Model)\n")
kable(model_summary1, digits = 4, caption = "coefficients table")

cat("\n Model Performance Metrics\n")
kable(model_glance1, digits = 4, caption = "Model Performance Metrics")

###

cat("\n=== No-HDL Model ===\n")
lm_model2 <- lm(log_hsCRP ~ PAL + Age +
                        Cholesterol + BMI  + CVD + Sex  ,
                    data = clean_data)
model_summary2 <- tidy(lm_model2)
model_glance2 <- glance(lm_model2)
# coefficient 
cat(" coefficients table  (No-HDL Model)\n")
kable(model_summary2, digits = 4, caption = "coefficients table")

cat("\n Model Performance Metrics\n")
kable(model_glance2, digits = 4, caption = "Model Performance Metrics")
```

- The **final linear regression model** (excluding HDL and DBP) was obtained through **backward elimination**.

- Significant predictors of **log-transformed hsCRP** included:

 - **Physical Activity Level (PAL)**
 - **Age**
 - **Sex**
 - **Cholesterol**
 - **BMI**
 - **CVD**

- All predictors in this model were **statistically significant (p < 0.2)**.


- The model achieved an **adjusted R² of 0.0845**, indicating that approximately **8.5% of the variability** in log(hsCRP) was explained by the selected predictors.

- Although the explanatory power remains relatively low, the model provides a **parsimonious and interpretable representation** of the associations between hsCRP and relevant clinical/lifestyle variables.

- The estimated coefficient for **BMI** was **0.0436**.

Interpretation: for each one-unit increase in BMI, **log(hsCRP)** increases by 0.0436, **holding all other predictors constant**.

When transformed back to the original scale, this corresponds to an approximate **4.5% increase in hsCRP levels per unit increase in BMI**, assuming all other variables remain fixed.

# **Model Building (Binary Logistic Regression)**

We preprocess hsCRP values by setting negatives to zero, capping extremes, and creating a binary variable using a **3 mg/L threshold**. A Binary Logistic Regression model then predicts high inflammation using age, sex, BMI, and other predictors, (The **reference group** includes individuals with **hsCRP ≤ 3 mg/L**, and the model estimates the likelihood of belonging to the hsCRP > 3 mg/L group.) [@hosmer2013applied].

```{r, echo=FALSE, warning=FALSE, message=FALSE}

# **Model Building ( Binary Logistic Regression )**
options(digits = 3)
# replaceing
clean_data$hsCRP <- ifelse(clean_data$hsCRP < 0, 0, clean_data$hsCRP)
# winsorize 
clean_data$hsCRP <- winsorize_var(clean_data$hsCRP, 0, 8)

# threshold 
threshold <- 3  
clean_data$hsCRP_binary <- ifelse(clean_data$hsCRP > threshold, 1, 0)

# --- MODEL BUILDING with hsCRP_binary ---
cat("\n=== Binary Logistic Regression Model ===\n")
bm_model <- glm(hsCRP_binary ~ Age + Sex + BMI + Cholesterol 
                + HDL + DBP + PAL + CVD,
               data = clean_data, family = binomial)
cat("\n glm(hsCRP_binary ~ Age + Sex + BMI + Cholesterol 
                + HDL + DBP + PAL + CVD,
               data = clean_data, family = binomial \n")

# --- MODEL PERFORMANCE EVALUATION ---
library(pROC)
library(ResourceSelection)
library(caret)
library(broom)
library(knitr)

# 1. Generate predicted probabilities
probabilities <- predict(bm_model, type = "response")

# 2. Calculate AUC
roc_obj <- roc(clean_data$hsCRP_binary, probabilities)
auc_value <- auc(roc_obj)

# 3. Create binary predictions
predictions <- ifelse(probabilities > 0.5, 1, 0)

# 4. Calculate confusion matrix
conf_matrix <- confusionMatrix(as.factor(predictions), 
                              as.factor(clean_data$hsCRP_binary))

# 5. Calculate Hosmer-Lemeshow test
hl_test <- hoslem.test(clean_data$hsCRP_binary, probabilities, g = 10)

# 6. Calculate Misclassification Rate
misclassification_rate <- 1 - conf_matrix$overall["Accuracy"]

performance_metrics <- data.frame(
  Metric = c("AIC", "AUC", "Accuracy", "Misclassification Rate",
             "Sensitivity", "Specificity", "Precision", "F1-Score",
             "Hosmer-Lemeshow Chi-square", "Hosmer-Lemeshow p-value"),
  Value = c(round(AIC(bm_model), 2),  # AIC
            round(auc_value, 4),
            round(conf_matrix$overall["Accuracy"], 4),
            round(misclassification_rate, 4),
            round(conf_matrix$byClass["Sensitivity"], 4),
            round(conf_matrix$byClass["Specificity"], 4),
            round(conf_matrix$byClass["Precision"], 4),
            round(conf_matrix$byClass["F1"], 4),
            round(hl_test$statistic, 4),
            round(hl_test$p.value, 4))
)

# --- DISPLAY ONLY WHAT YOU WANT ---
cat("\n Model Performance Metrics (hsCRP > 3 mg/L)\n")
kable(performance_metrics, digits = 4)

cat("\n Confusion Matrix\n")
print(conf_matrix$table)

```

**Binary Logistic Regression Model Performance**

- **Sensitivity (0.9543):**

  The model demonstrates a **strong ability to detect positive cases**, successfully identifying nearly all true events of interest.

- **Specificity (0.1489):**

  The model performs poorly in identifying negative cases, leading to a large number of false positives. Specifically, the confusion matrix shows 2,841 instances where the model incorrectly predicted "1" when the actual outcome was "0".

- **Accuracy (70.05%):**

  While the overall accuracy appears moderate, it is **misleading due to severe imbalance** between sensitivity and specificity.

- **AUC (0.6532):**

  The Area Under the ROC Curve indicates **only fair discriminative power**, suggesting the model has limited ability to distinguish between positive and negative cases.

- **F1-Score (0.8136):**

  Despite the imbalance, the relatively high F1-score reflects a **reasonable balance between precision and sensitivity** for the positive class.

- **Calibration (Hosmer–Lemeshow test, p = 0.2047):**

  Since the p-value is greater than 0.05, the model does not show significant lack of fit and appears to be **well-calibrated to the observed data**.


## **Model Comparison (Binary Logistic Regression)**

To evaluate the performance of the **Binary Logistic Regression** models, we compare them across two main dimensions:

1- **Model Fit (AIC):**

The Akaike Information Criterion (AIC) is used to assess the relative quality of the models, with lower values indicating a better fit to the data.

2- **Classification Performance:**

We assess the predictive ability of each model based on **Accuracy, Sensitivity, and Specificity**. These metrics provide complementary insights:

- **Accuracy** reflects the overall proportion of correctly classified cases.

- **Sensitivity** evaluates the ability of the model to correctly identify positive outcomes (true events).

- **Specificity** evaluates the ability to correctly identify negative outcomes, which is particularly important for reducing false positives.

By considering both model fit and predictive performance, we aim to identify the **best performing model** that balances statistical adequacy with practical classification ability.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# Updated binary model evaluation code with CI and OR (comments in English)

library(pROC)
library(caret)
library(broom)
library(knitr)

set.seed(123)

# ensure hsCRP has no negative values and winsorize (assumes winsorize_var exists)
clean_data$hsCRP <- ifelse(clean_data$hsCRP < 0, 0, clean_data$hsCRP)
clean_data$hsCRP <- winsorize_var(clean_data$hsCRP, 0, 8)

# threshold for binary outcome
threshold <- 3
clean_data$hsCRP_binary <- ifelse(clean_data$hsCRP > threshold, 1, 0)

# --- FUNCTION TO CREATE ENHANCED COEFFICIENTS TABLE ---
create_enhanced_table <- function(model) {
  # Get coefficients with confidence intervals (not exponentiated)
  tidy_model <- tidy(model, conf.int = TRUE, exponentiate = FALSE)
  
  enhanced_table <- data.frame(
    Term = tidy_model$term,
    Coefficient = round(tidy_model$estimate, 4),
    Std.Error = round(tidy_model$std.error, 4),
    Coefficient_CI_95 = paste0("(", round(tidy_model$conf.low, 4), ", ", round(tidy_model$conf.high, 4), ")"),
    Odds_Ratio = round(exp(tidy_model$estimate), 4),
    OR_CI_95 = paste0("(", round(exp(tidy_model$conf.low), 4), ", ", round(exp(tidy_model$conf.high), 4), ")"),
    Statistic = round(tidy_model$statistic, 4),
    P_value = round(tidy_model$p.value, 4)
  )
  return(enhanced_table)
}

# --- MODEL BUILDING with hsCRP_binary ---
# full model
bm_model0 <- glm(hsCRP_binary ~ Age + Sex + BMI + Cholesterol + HDL + DBP + PAL + CVD,
                 data = clean_data, family = binomial)

# Predicted probabilities
probabilities <- predict(bm_model0, type = "response")

# Create binary predictions (0.5 cutoff)
predictions <- ifelse(probabilities > 0.5, 1, 0)

# Calculate AUC
roc_obj <- roc(clean_data$hsCRP_binary, probabilities)
auc_value <- as.numeric(auc(roc_obj))

# Calculate confusion matrix and accuracy / sensitivity / specificity
# make sure factor levels match (0,1)
pred_fac <- factor(predictions, levels = c(0,1))
ref_fac  <- factor(clean_data$hsCRP_binary, levels = c(0,1))

conf_matrix <- confusionMatrix(pred_fac, ref_fac, positive = "1")

accuracy    <- as.numeric(conf_matrix$overall["Accuracy"])
sensitivity <- as.numeric(conf_matrix$byClass["Sensitivity"])
specificity <- as.numeric(conf_matrix$byClass["Specificity"])

# Calculate AIC for the model
model_aic <- AIC(bm_model0)

# Create performance metrics table (rounded)
performance_metrics <- data.frame(
  Metric = c("AUC", "Accuracy", "Sensitivity", "Specificity", "AIC"),
  Value = c(round(auc_value, 2),
            round(accuracy, 2),
            round(sensitivity, 2),
            round(specificity, 2),
            round(model_aic, 5))
)

# --- DISPLAY RESULTS ---
# Enhanced coefficients table
cat("Coefficients Table with CI and OR (Full Binary Model)\n")
kable(create_enhanced_table(bm_model0), digits = 4, 
      caption = "Coefficients Table with Confidence Intervals and Odds Ratios")

# Performance metrics
cat("\nModel Performance Metrics\n")
kable(performance_metrics, digits = 4, caption = "Model Performance Metrics")

# Confusion matrix
cat("\nConfusion Matrix (rows = Predictions, cols = Reference)\n")
print(conf_matrix$table)


# no-sex model
bm_model1 <- glm(hsCRP_binary ~ Age + BMI + Cholesterol + HDL + DBP + PAL + CVD,
                 data = clean_data, family = binomial)

# Predicted probabilities
probabilities <- predict(bm_model1, type = "response")

# Create binary predictions (0.5 cutoff)
predictions <- ifelse(probabilities > 0.5, 1, 0)

# Calculate AUC
roc_obj <- roc(clean_data$hsCRP_binary, probabilities)
auc_value <- as.numeric(auc(roc_obj))

# Calculate confusion matrix and accuracy / sensitivity / specificity
# make sure factor levels match (0,1)
pred_fac <- factor(predictions, levels = c(0,1))
ref_fac  <- factor(clean_data$hsCRP_binary, levels = c(0,1))

conf_matrix <- confusionMatrix(pred_fac, ref_fac, positive = "1")

accuracy    <- as.numeric(conf_matrix$overall["Accuracy"])
sensitivity <- as.numeric(conf_matrix$byClass["Sensitivity"])
specificity <- as.numeric(conf_matrix$byClass["Specificity"])

# Calculate AIC for the model
model_aic <- AIC(bm_model1)

# Create performance metrics table (rounded)
performance_metrics <- data.frame(
  Metric = c("AUC", "Accuracy", "Sensitivity", "Specificity", "AIC"),
  Value = c(round(auc_value, 2),
            round(accuracy, 2),
            round(sensitivity, 2),
            round(specificity, 2),
            round(model_aic, 5))
)

# --- DISPLAY RESULTS ---
# Enhanced coefficients table
cat("Coefficients Table with CI and OR (no-sex model)\n")
kable(create_enhanced_table(bm_model1), digits = 4, 
      caption = "Coefficients Table with Confidence Intervals and Odds Ratios")

# Performance metrics
cat("\nModel Performance Metrics\n")
kable(performance_metrics, digits = 4, caption = "Model Performance Metrics")

# Confusion matrix
cat("\nConfusion Matrix (rows = Predictions, cols = Reference)\n")
print(conf_matrix$table)

# no-DBP model
bm_model2 <- glm(hsCRP_binary ~ Age + BMI + Cholesterol + HDL + PAL + CVD,
                 data = clean_data, family = binomial)

# Predicted probabilities
probabilities <- predict(bm_model2, type = "response")

# Create binary predictions (0.5 cutoff)
predictions <- ifelse(probabilities > 0.5, 1, 0)

# Calculate AUC
roc_obj <- roc(clean_data$hsCRP_binary, probabilities)
auc_value <- as.numeric(auc(roc_obj))

# Calculate confusion matrix and accuracy / sensitivity / specificity
# make sure factor levels match (0,1)
pred_fac <- factor(predictions, levels = c(0,1))
ref_fac  <- factor(clean_data$hsCRP_binary, levels = c(0,1))

conf_matrix <- confusionMatrix(pred_fac, ref_fac, positive = "1")

accuracy    <- as.numeric(conf_matrix$overall["Accuracy"])
sensitivity <- as.numeric(conf_matrix$byClass["Sensitivity"])
specificity <- as.numeric(conf_matrix$byClass["Specificity"])

# Calculate AIC for the model
model_aic <- AIC(bm_model2)

# Create performance metrics table (rounded)
performance_metrics <- data.frame(
  Metric = c("AUC", "Accuracy", "Sensitivity", "Specificity", "AIC"),
  Value = c(round(auc_value, 2),
            round(accuracy, 2),
            round(sensitivity, 2),
            round(specificity, 2),
            round(model_aic, 5))
)

# --- DISPLAY RESULTS ---
# Enhanced coefficients table
cat("Coefficients Table with CI and OR (no-DBP model)\n")
kable(create_enhanced_table(bm_model2), digits = 4, 
      caption = "Coefficients Table with Confidence Intervals and Odds Ratios")

# Performance metrics
cat("\nModel Performance Metrics\n")
kable(performance_metrics, digits = 4, caption = "Model Performance Metrics")

# Confusion matrix
cat("\nConfusion Matrix (rows = Predictions, cols = Reference)\n")
print(conf_matrix$table)

# no-HDL model
bm_model3 <- glm(hsCRP_binary ~ Age + BMI + Cholesterol + PAL + CVD,
                 data = clean_data, family = binomial)

# Predicted probabilities
probabilities <- predict(bm_model3, type = "response")

# Create binary predictions (0.5 cutoff)
predictions <- ifelse(probabilities > 0.5, 1, 0)

# Calculate AUC
roc_obj <- roc(clean_data$hsCRP_binary, probabilities)
auc_value <- as.numeric(auc(roc_obj))

# Calculate confusion matrix and accuracy / sensitivity / specificity
# make sure factor levels match (0,1)
pred_fac <- factor(predictions, levels = c(0,1))
ref_fac  <- factor(clean_data$hsCRP_binary, levels = c(0,1))

conf_matrix <- confusionMatrix(pred_fac, ref_fac, positive = "1")

accuracy    <- as.numeric(conf_matrix$overall["Accuracy"])
sensitivity <- as.numeric(conf_matrix$byClass["Sensitivity"])
specificity <- as.numeric(conf_matrix$byClass["Specificity"])

# Calculate AIC for the model
model_aic <- AIC(bm_model3)

# Create performance metrics table (rounded)
performance_metrics <- data.frame(
  Metric = c("AUC", "Accuracy", "Sensitivity", "Specificity", "AIC"),
  Value = c(round(auc_value, 2),
            round(accuracy, 2),
            round(sensitivity, 2),
            round(specificity, 2),
            round(model_aic, 5))
)

# --- DISPLAY RESULTS ---
# Enhanced coefficients table
cat("Coefficients Table with CI and OR (no-HDL model)\n")
kable(create_enhanced_table(bm_model3), digits = 4, 
      caption = "Coefficients Table with Confidence Intervals and Odds Ratios")

# Performance metrics
cat("\nModel Performance Metrics\n")
kable(performance_metrics, digits = 4, caption = "Model Performance Metrics")

# Confusion matrix
cat("\nConfusion Matrix (rows = Predictions, cols = Reference)\n")
print(conf_matrix$table)
```

The **final  model** estimates the likelihood of having **having high inflammation** (hsCRP > 3 mg/L). The interpretation of the **odds ratios (OR)** for each predictor is as follows:

**Coefficients Interpretation**

- **Age:** 

  Each additional year of age increases the odds of high inflammation by approximately **1%** (OR = 1.010).

- **BMI:**

  A one-unit increase in BMI raises the odds of high inflammation by about **12%** (OR = 1.117).

- **Cholesterol:**

  Each unit increase in cholesterol is associated with a **0.5%** increase in risk (OR =1.005)

- **Physical Activity (PAL):**

  Higher physical activity levels are unexpectedly associated with a **68% increase** in odds of high inflammation (OR = 1.677). This counterintuitive finding warrants **further investigation**.

- **Heart Disease (CVD):**

  Individuals with cardiovascular disease have about a **43% higher risk** of high inflammation (OR = 1.428)


**Model Performance Metrics**

- **Accuracy:**

  The model correctly classifies outcomes ~**70%** of the time.

- **Specificity (Low Inflammation):**

T  he model performs **very well** in identifying individuals with low inflammation **(95% correct)**

- **Sensitivity (High Inflammation):**

  The model performs **poorly** in identifying individuals with high inflammation **(only 15% correct)**.

**Overall Performance:**

While the model shows **high specificity**, its **low sensitivity** limits its usefulness in detecting individuals at risk of high inflammation. In a **health screening context**, this means the model would miss many true high-risk cases, potentially leading to underdiagnosis.

# **Model Building (Standard Ordinal Logistic Regression)**

We now fit a **Standard Ordinal Logistic Regression** model (Proportional-Odds Model) because the outcome variable has **three ordered categories**:

- **Low (<1)**

- **Medium (1–3)**

- **High (>3)**

This type of model is suitable when categories have a natural order (low → medium → high), but the spacing between them is not necessarily equal.

**Interpretation**

The model estimates how each predictor influences the **odds of being in a higher inflammation category** (e.g., moving from Low to Medium, or from Medium to High), while holding all other predictors constant [@hosmer2013applied].

**Key Assumptions to Check**

1- **Proportional-Odds (Parallel Lines) Assumption**

- The effect of each predictor is assumed to be consistent across all thresholds of the outcome.

- Example: The effect of BMI on moving from Low → Medium is the same as its effect on moving from Medium → High.

2- **Model Fit**

- The model should adequately represent the observed data without systematic bias.

**Next Step**

Before finalizing interpretation, we will run a statistical test for the **parallel lines assumption**. If this assumption does not hold, alternative modeling approaches (e.g., partial proportional-odds models or multinomial logistic regression) may be considered.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(MASS)
library(pscl)
library(pROC)
library(caret)
library(car)
library(lmtest)
library(knitr)   
library(dplyr)
library(carData)
# --- Data preparation ---
clean_data$hsCRP_ord <- cut(clean_data$hsCRP, 
                            breaks = c(-Inf, 1, 3, Inf), 
                            labels = c("Low", "Medium", "High"),
                            ordered_result = TRUE)

# --- Ordinal regression model ---
ord_model <- polr(hsCRP_ord ~ PAL + Age + DBP + Cholesterol + BMI + HDL + CVD + Sex,
                  data = clean_data, Hess = TRUE)

# --- Coefficients table with OR and CI ---
ctab <- coef(summary(ord_model))
pvals <- pnorm(abs(ctab[, "t value"]), lower.tail = FALSE) * 2

# Calculate odds ratios
odds_ratio <- exp(ctab[, "Value"])

# Calculate confidence intervals
ci <- exp(confint(ord_model))

# Combine all information
ctab_full <- cbind(
  ctab,
  "p.value" = round(pvals, 4),
  "OR" = round(odds_ratio, 4),
  "CI 2.5%" = round(ci[, 1], 4),
  "CI 97.5%" = round(ci[, 2], 4)
)

kable(ctab_full, caption = "Coefficients Table with OR and CI (Ordinal Logistic Regression)")

# --- Performance metrics with additional measures ---
probabilities <- predict(ord_model, type = "probs")
predictions <- predict(ord_model, type = "class")

# multiclass AUC
actual_numeric <- as.numeric(clean_data$hsCRP_ord)
predicted_numeric <- as.numeric(predictions)
multiclass_roc <- multiclass.roc(actual_numeric, predicted_numeric)
multiclass_auc <- multiclass_roc$auc

# Accuracy
conf_matrix <- confusionMatrix(predictions, clean_data$hsCRP_ord)
accuracy <- conf_matrix$overall["Accuracy"]

# Pseudo R2
pseudo_r2 <- tryCatch({
  pR2(ord_model)["McFadden"]
}, error = function(e) NA_real_)

# Calculate -2*log-likelihood and AIC
loglik <- -2 * as.numeric(logLik(ord_model))
aic <- AIC(ord_model)

# all in a table
perf_metrics <- data.frame(
  Metric = c("Multiclass AUC", "Accuracy", "Pseudo R² (McFadden)", 
             "-2*Log-Likelihood", "AIC"),
  Value  = c(
    round(as.numeric(multiclass_auc), 4), 
    round(as.numeric(accuracy), 4), 
    ifelse(is.na(pseudo_r2), NA, round(as.numeric(pseudo_r2), 4)),
    round(loglik, 2),
    round(aic, 2)
  )
)
kable(perf_metrics, caption = "Model Performance Metrics")

# --- Tests ---

#  Proportional Odds assumption test
cat("\n Test of Parallel Lines\n") 

car::poTest(ord_model)


# 2. Likelihood ratio test
null_model <- polr(hsCRP_ord ~ 1, data = clean_data, Hess = TRUE)

# Manual likelihood ratio test calculation
ll_null <- as.numeric(logLik(null_model))
ll_full <- as.numeric(logLik(ord_model))
lr_statistic <- -2 * (ll_null - ll_full)
df_diff <- attr(logLik(ord_model), "df") - attr(logLik(null_model), "df")
lr_pvalue <- pchisq(lr_statistic, df = df_diff, lower.tail = FALSE)

# Create results table
lr_test_tbl <- data.frame(
  Model_Comparison = "Null vs Full Model",
  Chi_Square = round(lr_statistic, 2),
  df = df_diff,
  p_value = round(lr_pvalue, 4)
)

kable(lr_test_tbl, caption = "Likelihood Ratio Test: Null vs Full Model")

# --- Confusion Matrix ---
cat("\n Confusion Matrix\n")
conf_matrix_df <- as.data.frame.matrix(conf_matrix$table)
conf_matrix_df <- cbind(Actual = rownames(conf_matrix_df), conf_matrix_df)
rownames(conf_matrix_df) <- NULL
kable(conf_matrix_df, caption = "Confusion Matrix")
```

The **ordinal logistic regression model** was fitted to predict ordered categories of **hsCRP** (Low < 1, Medium 1–3, High > 3).

**Model Performance**

- **Multiclass AUC:** 0.603

- **Overall accuracy:** 0.45

- **McFadden’s pseudo-R²:** 0.045

These values indicate **modest performance** and limited explanatory power. However, **the likelihood-ratio test** comparing the null and full model was highly significant (χ² = 1028, df = 8, p < 0.001), confirming that the predictors jointly improve the fit relative to the null model.

**Assumption Testing (Proportional-Odds / Parallel Lines)**

- The **Brant test** rejected the proportional-odds assumption (χ² = 29.55, df = 8, p = 0.00025).

- **Item-level violations:**

 - **CVD:** p = 0.0056 (non-proportional)

 - **HDL:** p = 0.0297 (non-proportional)

 - **PAL:** borderline (p ≈ 0.050)

This result indicates that the effect of these predictors is **not constant across outcome thresholds** (e.g., their effect on Low→Medium differs from Medium→High).

**Classification Patterns**

The **confusion matrix** showed that the model tends to **overpredict the middle (Medium) category**, while sensitivity for the **High** category was limited. This suggests that predictive accuracy varies substantially across categories and should be considered when interpreting the results.

**Next Step**

Because the **parallel-lines assumption is violated**, the **standard proportional-odds model is not strictly appropriate**. A more flexible alternative is the **Generalized Ordinal Logistic Regression model**, which relaxes the assumption and allows **coefficients to vary across thresholds**. This approach provides a better framework for handling predictors with non-proportional effects and is recommended for further analysis.

# **Model Building (Generalized Ordinal Logistic Regression)**

To address the violation of the proportional-odds assumption, we fit a **Generalized Ordinal Logistic Regression model** that **fully relaxes** the parallel-lines constraint. In this specification, each predictor is allowed to have **different coefficients at each threshold** (Low → Medium and Medium → High). This means that the effect of a variable (e.g., CVD, HDL) may differ depending on which transition is being modeled.

**Planned Steps**

1- **Fit the full non-parallel model**

- All predictors are estimated with separate coefficients across thresholds.

2- **Model comparison**

- Compare the generalized model to the standard proportional-odds model using **AIC** and **likelihood-ratio tests**.

- This establishes whether the added flexibility yields a meaningful improvement in fit.

3- **Report results**

- Present threshold-specific odds ratios (ORs) with 95% confidence intervals.

- Provide classification diagnostics, including:

 - Multiclass AUC

 - Overall accuracy

 - Confusion matrix

**Interpretation Strategy**

- Emphasis will be placed on predictors that show **substantively different effects across thresholds**.

 - Example: **CVD or HDL**, which were flagged in the Brant test as non-proportional.

- Assess whether the **non-parallel model improves predictive performance** compared to the proportional-odds model.

- Discuss whether the added complexity is justified by clearer or more accurate insights into the factors influencing hsCRP categories.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(VGAM)
library(MASS)
library(knitr)
library(dplyr)
library(pROC)
library(caret)
library(lmtest)
library(brant)

# --- Data Preparation ---
clean_data$hsCRP_ord <- cut(clean_data$hsCRP, 
                            breaks = c(-Inf, 1, 3, Inf), 
                            labels = c("Low", "Medium", "High"),
                            ordered_result = TRUE)

# --- Generalized Ordinal Logistic Regression ---
gen_ord_model <- vglm(hsCRP_ord ~ PAL + Age + DBP + Cholesterol + BMI + HDL + CVD + Sex,
                      family = cumulative(parallel = FALSE, reverse = FALSE),
                      data = clean_data)

# --- Coefficients Table ---
coef_summary <- summary(gen_ord_model)@coef3
odds_ratio <- exp(coef_summary[, "Estimate"])
ci_lower <- exp(coef_summary[, "Estimate"] - 1.96 * coef_summary[, "Std. Error"])
ci_upper <- exp(coef_summary[, "Estimate"] + 1.96 * coef_summary[, "Std. Error"])

variable_names <- rownames(coef_summary)
comparison_levels <- ifelse(grepl(":1", variable_names), "Low vs Medium", 
                           ifelse(grepl(":2", variable_names), "Low+Medium vs High ", "Intercept"))
clean_variable_names <- gsub(":1", "", variable_names)
clean_variable_names <- gsub(":2", "", clean_variable_names)

coefficients_table <- data.frame(
  Variable = clean_variable_names,
  Comparison = comparison_levels,
  Coefficient = round(coef_summary[, "Estimate"], 4),
  Std_Error = round(coef_summary[, "Std. Error"], 4),
  Odds_Ratio = round(odds_ratio, 4),
  CI_2.5 = round(ci_lower, 4),
  CI_97.5 = round(ci_upper, 4),
  p_value = round(coef_summary[, "Pr(>|z|)"], 4)
)

coefficients_table$Variable <- ifelse(duplicated(coefficients_table$Variable), 
                                     "", coefficients_table$Variable)

kable(coefficients_table, caption = "Generalized Ordinal Logistic Regression Coefficients")

# --- Performance Metrics ---
predictions <- predict(gen_ord_model, type = "response")
predicted_classes <- apply(predictions, 1, which.max)
predicted_classes <- factor(predicted_classes, levels = 1:3, 
                           labels = c("Low", "Medium", "High"))

conf_matrix <- confusionMatrix(predicted_classes, clean_data$hsCRP_ord)

sensitivity_values <- conf_matrix$byClass[,"Sensitivity"]
specificity_values <- conf_matrix$byClass[,"Specificity"]
class_names <- c("Low", "Medium", "High")

# FIXED: Pseudo R-squared calculation
null_model <- vglm(hsCRP_ord ~ 1, family = cumulative(parallel = FALSE), data = clean_data)
pseudo_r2 <- 1 - (as.numeric(logLik(gen_ord_model)) / as.numeric(logLik(null_model)))

# Calculate multiclass AUC
auc_value <- multiclass.roc(clean_data$hsCRP_ord, as.numeric(predicted_classes))$auc

performance_metrics <- data.frame(
  Metric = c("Accuracy", "Pseudo R-squared", "AIC", "AUC",
             paste0("Sensitivity (", class_names, ")"),
             paste0("Specificity (", class_names, ")")),
  Value = c(round(conf_matrix$overall["Accuracy"], 4),
            round(pseudo_r2, 4),
            round(AIC(gen_ord_model), 2),
            round(auc_value, 4),
            round(sensitivity_values, 4),
            round(specificity_values, 4))
)

kable(performance_metrics, caption = "Model Performance Metrics")

# --- Confusion Matrix ---
conf_matrix_df <- as.data.frame.matrix(conf_matrix$table)
conf_matrix_df <- cbind(Actual = rownames(conf_matrix_df), conf_matrix_df)
rownames(conf_matrix_df) <- NULL
kable(conf_matrix_df, caption = "Confusion Matrix")

# --- Statistical Tests ---
# Likelihood Ratio Test
loglik_null <- as.numeric(logLik(null_model))
loglik_full <- as.numeric(logLik(gen_ord_model))
df_diff <- length(coef(gen_ord_model)) - length(coef(null_model))
lr_statistic <- -2 * (loglik_null - loglik_full)
lr_pvalue <- pchisq(lr_statistic, df = df_diff, lower.tail = FALSE)

lr_test_table <- data.frame(
  Test = "Likelihood Ratio Test",
  Chi_Square = round(lr_statistic, 2),
  df = df_diff,
  p_value = round(lr_pvalue, 4)
)

kable(lr_test_table, caption = "Likelihood Ratio Test Results")

# --- Brant Test (FIXED) ---
ord_model_polr <- polr(hsCRP_ord ~ PAL + Age + DBP + Cholesterol + BMI + HDL + CVD + Sex,
                      data = clean_data, Hess = TRUE)
brant_result <- brant(ord_model_polr)

# FIXED: Correct way to access brant results
brant_table <- data.frame(
  Test = "Brant Test",
  Chi_Square = round(as.numeric(brant_result[1]), 2),
  df = as.numeric(brant_result[2]),
  p_value = round(as.numeric(brant_result[3]), 4)
)

kable(brant_table, caption = "Brant Test for Proportional Odds")

# --- Correct Classification Matrix ---
correctly_classified <- diag(conf_matrix$table)
classification_matrix <- data.frame(
  Class = c("Low", "Medium", "High"),
  Correctly_Classified = correctly_classified,
  Total_Observations = colSums(conf_matrix$table),
  Percentage_Correct = round(correctly_classified / colSums(conf_matrix$table) * 100, 2)
)

kable(classification_matrix, caption = "Correct Classification Matrix")
```

**Summary of Generalized Ordinal Logistic Model**

The **generalized ordinal logistic regression model** provided a significantly better fit than the null model (Likelihood-Ratio test). However, its **explanatory power remained limited** (McFadden’s pseudo-R² ≈ 0.046) and the **overall accuracy was moderate (~0.46)**.

Importantly, the **proportional-odds assumption was violated** for some predictors, particularly **CVD** and **HDL**, indicating that their effects differ across outcome thresholds. Accordingly, **threshold-specific coefficients** were reported for these variables.

In terms of classification, the model performed **best for the Medium category**, while prediction of the Low and High categories was weaker.

**Next Steps**

In the following section, we will conduct a formal model comparison using:

- **AIC**

- **Multiclass AUC**

- **Classification metrics** (e.g., sensitivity and specificity by category)

This will help determine which modeling approach provides the most reliable balance of fit and predictive performance.

## **Model Comparison (Generalized Ordinal Logistic Regression)**

At this stage, we **compare generalized ordinal logistic regression models** to **identify the best fitting and most parsimonious specification**.

**Evaluation Criteria**

- **Model fit:**

 - **AIC** (Akaike Information Criterion)

 - **Pseudo R²** measures

- **Predictive performance:**

 - **Overall accuracy**

 - **Multiclass AUC**

 - **Class-specific sensitivity and specificity**

**Rationale**

This comparison process ensures that the selected model:

1- **Adequately explains** the relationship between predictors and the ordinal outcome.

2- Provides **reliable predictive performance** across all categories (Low, Medium, High).

3- Avoids **overfitting**, by balancing flexibility with parsimony.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(VGAM)
library(MASS)
library(knitr)
library(dplyr)
# --- Data Preparation ---
clean_data$hsCRP_ord <- cut(clean_data$hsCRP, 
                            breaks = c(-Inf, 1, 3, Inf), 
                            labels = c("Low", "Medium", "High"),
                            ordered_result = TRUE)

cat("\n Full Model \n")
# --- Generalized Ordinal Logistic Regression (FULL MODEL) ---
gen_ord_model_full <- vglm(hsCRP_ord ~ PAL + Age + DBP + Cholesterol + BMI + HDL + CVD + Sex,
                      family = cumulative(parallel = FALSE, reverse = FALSE),
                      data = clean_data)

# --- Coefficients Table for FULL MODEL---
coef_summary_full <- summary(gen_ord_model_full)@coef3
odds_ratio_full <- exp(coef_summary_full[, "Estimate"])
ci_lower_full <- exp(coef_summary_full[, "Estimate"] - 1.96 * coef_summary_full[, "Std. Error"])
ci_upper_full <- exp(coef_summary_full[, "Estimate"] + 1.96 * coef_summary_full[, "Std. Error"])

variable_names_full <- rownames(coef_summary_full)
comparison_levels_full <- ifelse(grepl(":1", variable_names_full), "Low vs Medium", 
                           ifelse(grepl(":2", variable_names_full), "Low+Medium vs High", "Intercept"))
clean_variable_names_full <- gsub(":1", "", variable_names_full)
clean_variable_names_full <- gsub(":2", "", clean_variable_names_full)

coefficients_table_full <- data.frame(
  Variable = clean_variable_names_full,
  Comparison = comparison_levels_full,
  Coefficient = round(coef_summary_full[, "Estimate"], 4),
  Std_Error = round(coef_summary_full[, "Std. Error"], 4),
  Odds_Ratio = round(odds_ratio_full, 4),
  CI_2.5 = round(ci_lower_full, 4),
  CI_97.5 = round(ci_upper_full, 4),
  p_value = round(coef_summary_full[, "Pr(>|z|)"], 4)
)

coefficients_table_full$Variable <- ifelse(duplicated(coefficients_table_full$Variable), 
                                     "", coefficients_table_full$Variable)

kable(coefficients_table_full, caption = "Generalized Ordinal Logistic Regression Coefficients - Full Model")

# --- Model Comparison Metrics for FULL MODEL---
# 1. AIC
aic_value_full <- AIC(gen_ord_model_full)

# 2. Pseudo R-squared (McFadden)
null_model <- vglm(hsCRP_ord ~ 1, family = cumulative(parallel = FALSE), data = clean_data)
pseudo_r2_full <- 1 - (as.numeric(logLik(gen_ord_model_full)) / as.numeric(logLik(null_model)))

# 3. Likelihood Ratio Test for FULL MODEL
loglik_null <- as.numeric(logLik(null_model))
loglik_full_model <- as.numeric(logLik(gen_ord_model_full))
df_diff_full <- length(coef(gen_ord_model_full)) - length(coef(null_model))
lr_statistic_full <- -2 * (loglik_null - loglik_full_model)
lr_pvalue_full <- pchisq(lr_statistic_full, df = df_diff_full, lower.tail = FALSE)

# --- Display Results for FULL MODEL---
cat("\n=== FULL MODEL RESULTS ===\n")
# AIC
cat("AIC:", round(aic_value_full, 2), "\n\n")

# Pseudo R-squared
cat("Pseudo R-squared (McFadden):", round(pseudo_r2_full, 4), "\n\n")

# Likelihood Ratio Test Results
lr_test_table_full <- data.frame(
  Test = "Likelihood Ratio Test (Full vs Null)",
  Chi_Square = round(lr_statistic_full, 2),
  df = df_diff_full,
  p_value = round(lr_pvalue_full, 4)
)

kable(lr_test_table_full, caption = "Likelihood Ratio Test: Null vs Full Model")

######### no-sex model
cat("\n No-Sex Model \n")
gen_ord_model_no_sex <- vglm(hsCRP_ord ~ PAL + Age + DBP + Cholesterol + BMI + HDL + CVD,
                      family = cumulative(parallel = FALSE, reverse = FALSE),
                      data = clean_data)

# --- Coefficients Table for NO-SEX MODEL---
coef_summary_nosex <- summary(gen_ord_model_no_sex)@coef3
odds_ratio_nosex <- exp(coef_summary_nosex[, "Estimate"])
ci_lower_nosex <- exp(coef_summary_nosex[, "Estimate"] - 1.96 * coef_summary_nosex[, "Std. Error"])
ci_upper_nosex <- exp(coef_summary_nosex[, "Estimate"] + 1.96 * coef_summary_nosex[, "Std. Error"])

variable_names_nosex <- rownames(coef_summary_nosex)
comparison_levels_nosex <- ifelse(grepl(":1", variable_names_nosex), "Medium vs Low", 
                           ifelse(grepl(":2", variable_names_nosex), "Low+Medium vs High", "Intercept"))
clean_variable_names_nosex <- gsub(":1", "", variable_names_nosex)
clean_variable_names_nosex <- gsub(":2", "", clean_variable_names_nosex)

coefficients_table_nosex <- data.frame(
  Variable = clean_variable_names_nosex,
  Comparison = comparison_levels_nosex,
  Coefficient = round(coef_summary_nosex[, "Estimate"], 4),
  Std_Error = round(coef_summary_nosex[, "Std. Error"], 4),
  Odds_Ratio = round(odds_ratio_nosex, 4),
  CI_2.5 = round(ci_lower_nosex, 4),
  CI_97.5 = round(ci_upper_nosex, 4),
  p_value = round(coef_summary_nosex[, "Pr(>|z|)"], 4)
)

coefficients_table_nosex$Variable <- ifelse(duplicated(coefficients_table_nosex$Variable), 
                                     "", coefficients_table_nosex$Variable)

kable(coefficients_table_nosex, caption = "Generalized Ordinal Logistic Regression Coefficients - No-Sex Model")

# --- Model Comparison Metrics for NO-SEX MODEL---
# 1. AIC
aic_value_nosex <- AIC(gen_ord_model_no_sex)

# 2. Pseudo R-squared (McFadden)
pseudo_r2_nosex <- 1 - (as.numeric(logLik(gen_ord_model_no_sex)) / as.numeric(logLik(null_model)))

# 3. Likelihood Ratio Test for NO-SEX MODEL
loglik_nosex <- as.numeric(logLik(gen_ord_model_no_sex))
df_diff_nosex <- length(coef(gen_ord_model_no_sex)) - length(coef(null_model))
lr_statistic_nosex <- -2 * (loglik_null - loglik_nosex)
lr_pvalue_nosex <- pchisq(lr_statistic_nosex, df = df_diff_nosex, lower.tail = FALSE)

# --- Display Results for NO-SEX MODEL---
cat("\n=== NO-SEX MODEL RESULTS ===\n")
# AIC
cat("AIC:", round(aic_value_nosex, 2), "\n\n")

# Pseudo R-squared
cat("Pseudo R-squared (McFadden):", round(pseudo_r2_nosex, 4), "\n\n")

# Likelihood Ratio Test Results
lr_test_table_nosex <- data.frame(
  Test = "Likelihood Ratio Test (No-Sex vs Null)",
  Chi_Square = round(lr_statistic_nosex, 2),
  df = df_diff_nosex,
  p_value = round(lr_pvalue_nosex, 4)
)

kable(lr_test_table_nosex, caption = "Likelihood Ratio Test: Null vs No-Sex Model")

######### no-DBP model
cat("\n No-DBP Model \n")
gen_ord_model_no_dbp <- vglm(hsCRP_ord ~ PAL + Age + Cholesterol + BMI + HDL + CVD ,
                      family = cumulative(parallel = FALSE, reverse = FALSE),
                      data = clean_data)

# --- Coefficients Table for NO-DBP MODEL---
coef_summary_nodbp <- summary(gen_ord_model_no_dbp)@coef3
odds_ratio_nodbp <- exp(coef_summary_nodbp[, "Estimate"])
ci_lower_nodbp <- exp(coef_summary_nodbp[, "Estimate"] - 1.96 * coef_summary_nodbp[, "Std. Error"])
ci_upper_nodbp <- exp(coef_summary_nodbp[, "Estimate"] + 1.96 * coef_summary_nodbp[, "Std. Error"])

variable_names_nodbp <- rownames(coef_summary_nodbp)
comparison_levels_nodbp <- ifelse(grepl(":1", variable_names_nodbp), "Low vs Medium", 
                           ifelse(grepl(":2", variable_names_nodbp), "Low+Medium vs High", "Intercept"))
clean_variable_names_nodbp <- gsub(":1", "", variable_names_nodbp)
clean_variable_names_nodbp <- gsub(":2", "", clean_variable_names_nodbp)

coefficients_table_nodbp <- data.frame(
  Variable = clean_variable_names_nodbp,
  Comparison = comparison_levels_nodbp,
  Coefficient = round(coef_summary_nodbp[, "Estimate"], 4),
  Std_Error = round(coef_summary_nodbp[, "Std. Error"], 4),
  Odds_Ratio = round(odds_ratio_nodbp, 4),
  CI_2.5 = round(ci_lower_nodbp, 4),
  CI_97.5 = round(ci_upper_nodbp, 4),
  p_value = round(coef_summary_nodbp[, "Pr(>|z|)"], 4)
)

coefficients_table_nodbp$Variable <- ifelse(duplicated(coefficients_table_nodbp$Variable), 
                                     "", coefficients_table_nodbp$Variable)

kable(coefficients_table_nodbp, caption = "Generalized Ordinal Logistic Regression Coefficients - No-DBP Model")

# --- Model Comparison Metrics for NO-DBP MODEL---
# 1. AIC
aic_value_nodbp <- AIC(gen_ord_model_no_dbp)

# 2. Pseudo R-squared (McFadden)
pseudo_r2_nodbp <- 1 - (as.numeric(logLik(gen_ord_model_no_dbp)) / as.numeric(logLik(null_model)))

# 3. Likelihood Ratio Test for NO-DBP MODEL
loglik_nodbp <- as.numeric(logLik(gen_ord_model_no_dbp))
df_diff_nodbp <- length(coef(gen_ord_model_no_dbp)) - length(coef(null_model))
lr_statistic_nodbp <- -2 * (loglik_null - loglik_nodbp)
lr_pvalue_nodbp <- pchisq(lr_statistic_nodbp, df = df_diff_nodbp, lower.tail = FALSE)

# --- Display Results for NO-DBP MODEL---
cat("\n=== NO-DBP MODEL RESULTS ===\n")
# AIC
cat("AIC:", round(aic_value_nodbp, 2), "\n\n")

# Pseudo R-squared
cat("Pseudo R-squared (McFadden):", round(pseudo_r2_nodbp, 4), "\n\n")

# Likelihood Ratio Test Results
lr_test_table_nodbp <- data.frame(
  Test = "Likelihood Ratio Test (No-DBP vs Null)",
  Chi_Square = round(lr_statistic_nodbp, 2),
  df = df_diff_nodbp,
  p_value = round(lr_pvalue_nodbp, 4)
)

kable(lr_test_table_nodbp, caption = "Likelihood Ratio Test: Null vs No-DBP Model")

# --- Model Comparison Table ---
cat("\n=== MODEL COMPARISON TABLE ===\n")

# Function to calculate performance metrics for each model
calculate_model_metrics <- function(model, model_name) {
  # Predictions
  predictions <- predict(model, type = "response")
  predicted_classes <- apply(predictions, 1, which.max)
  predicted_classes <- factor(predicted_classes, levels = 1:3, 
                             labels = c("Low", "Medium", "High"))
  
  # Confusion Matrix
  conf_matrix <- confusionMatrix(predicted_classes, clean_data$hsCRP_ord)
  
  # AUC
  auc_value <- as.numeric(multiclass.roc(clean_data$hsCRP_ord, as.numeric(predicted_classes))$auc)
  
  # Correct Classification Matrix
  correctly_classified <- diag(conf_matrix$table)
  total_observations <- colSums(conf_matrix$table)
  percentage_correct <- correctly_classified / total_observations * 100
  
  # AIC
  aic_value <- AIC(model)
  
  # Pseudo R-squared
  pseudo_r2 <- 1 - (as.numeric(logLik(model)) / as.numeric(logLik(null_model)))
  
  # Return results
  return(data.frame(
    Model = model_name,
    AIC = round(aic_value, 2),
    AUC = round(auc_value, 4),
    Accuracy = round(conf_matrix$overall["Accuracy"], 4),
    Pseudo_R2 = round(pseudo_r2, 4),
    Low_Class_Percent = round(percentage_correct[1], 2),
    Medium_Class_Percent = round(percentage_correct[2], 2),
    High_Class_Percent = round(percentage_correct[3], 2),
    Avg_Class_Percent = round(mean(percentage_correct), 2)
  ))
}

# Calculate metrics for all models
comparison_table <- rbind(
  calculate_model_metrics(gen_ord_model_full, "Full Model"),
  calculate_model_metrics(gen_ord_model_no_sex, "No-Sex Model"),
  calculate_model_metrics(gen_ord_model_no_dbp, "No-DBP Model")
)

# Display comparison table
kable(comparison_table, caption = "Model Comparison based on AIC, AUC and Classification Performance")

# --- Detailed Correct Classification Matrices ---
cat("\n=== DETAILED CORRECT CLASSIFICATION MATRICES ===\n")

# Function to create classification matrix
create_classification_matrix <- function(model, model_name) {
  predictions <- predict(model, type = "response")
  predicted_classes <- apply(predictions, 1, which.max)
  predicted_classes <- factor(predicted_classes, levels = 1:3, 
                             labels = c("Low", "Medium", "High"))
  
  conf_matrix <- confusionMatrix(predicted_classes, clean_data$hsCRP_ord)
  correctly_classified <- diag(conf_matrix$table)
  
  classification_matrix <- data.frame(
    Model = model_name,
    Class = c("Low", "Medium", "High"),
    Correctly_Classified = correctly_classified,
    Total_Observations = colSums(conf_matrix$table),
    Percentage_Correct = round(correctly_classified / colSums(conf_matrix$table) * 100, 2)
  )
  
  return(classification_matrix)
}

# Create classification matrices for all models
classification_matrices <- rbind(
  create_classification_matrix(gen_ord_model_full, "Full Model"),
  create_classification_matrix(gen_ord_model_no_sex, "No-Sex Model"),
  create_classification_matrix(gen_ord_model_no_dbp, "No-DBP Model")
)

# Display classification matrices
kable(classification_matrices, caption = "Detailed Correct Classification Matrices for All Models")

```

**Interpretation**

**Variable Retention**

During model testing, removing **CVD** or **HDL** led to higher AIC values. Since each was statistically significant at least at one threshold, both variables were retained in the final model, even though their effects were weaker at other levels.


**Key Significant Predictors (Both Thresholds)**

**Physical Activity Level (PAL):**

- Low → Medium: OR = 0.739 (95% CI: 0.625–0.873)

- Medium → High: OR = 0.590 (95% CI: 0.502–0.693)

- Interpretation: The more active people are, the less likely they are to have high hsCRP. Physical activity clearly shows a protective effect.

**Age:**

- Low → Medium: OR = 0.987 (95% CI: 0.981–0.992)

- Medium → High: OR = 0.990 (95% CI: 0.985–0.995)

- Interpretation: Surprisingly, older age was linked to slightly lower hsCRP. This could be due to other factors such as medication use, lifestyle differences, or specific characteristics of the sample.

**Cholesterol:**

- Low → Medium: OR = 0.994 (95% CI: 0.993–0.995)

- Medium → High: OR = 0.995 (95% CI: 0.994–0.996)

- Interpretation: Higher cholesterol levels were associated with a higher chance of elevated hsCRP, even though the effect size was small.
**BMI:**

- Low → Medium: OR = 0.886 (95% CI: 0.876–0.896)

- Medium → High: OR = 0.896 (95% CI: 0.888–0.905)

- Interpretation: This was one of the strongest predictors. Higher BMI was strongly linked to higher inflammation, which is consistent with findings from other studies

**Partially Significant Predictors:**

**HDL:**

- Only significant for Medium → High (OR = 0.996, p = 0.096)

- Interpretation: The effect was very small. HDL didn’t show a strong protective role in this dataset.

**CVD:**

- Significant only for Medium → High (OR = 0.721, p = 0.021).

- Interpretation: Interestingly, people with CVD were less likely to be in the high hsCRP group. A possible explanation is that many of them take medications (like statins) that help reduce inflammation.

**Model Performance Metrics**

**AIC = 21,874** → Slightly better than alternatives (Full = 21,879; No-Sex = 21,876).

**Pseudo R² = 0.0462** → Low explanatory power (common in biomedical models with many unmeasured influences).

**Likelihood-ratio test:** χ² = 1058, df = 12, p < 0.001 → Predictors collectively improve fit.

**Accuracy = 0.455 (45.5%)** → Moderate classification ability.

**Multiclass AUC = 0.607** → Fair discrimination (typical for clinical prediction)

**Classification Matrix Insights**

- **Low category:** 22.7% correctly classified → weak performance.

- **Medium category:** 72.5% correctly classified → strongest performance.

- **High category:** 28.8% correctly classified → limited identification of high-risk cases

**Clinical and Research Implications**

- The model shows that **lifestyle and metabolic factors (PAL, BMI, cholesterol)** are consistently important for hsCRP levels.

- It performs best for identifying **medium-risk individuals**, but has weaker ability for low- and high-risk groups.

- **HDL and CVD** contribute modestly but improve the overall model fit, so their inclusion is justified.

- Clinically, results suggest that interventions targeting **physical activity, BMI, and cholesterol** are central for inflammation management.

- For research, future studies should include additional predictors to better capture low- and high-risk hsCRP cases.

- Overall, the model provides **useful insights rather than precise predictions**, highlighting both the promise and the limitations of current data.

# **Conclusion**

## **Choosing the Right Model for the Right Goal**

Our analysis highlights that there is no single “best” model. The choice of model depends on the **specific goal of the analysis**. Each approach has unique strengths and weaknesses, making it more or less suitable depending on the context.

**1. Linear Regression — Predicting an Exact Value**

- **Best for:** Estimating the precise hsCRP level for an individual.

- **Strength:** Results are simple to interpret. For example, “Each one-unit increase in BMI is associated with a 0.0449 mg/L increase in hsCRP.”

- **Limitation:** It does not account for medical cut-offs (1 and 3 mg/L). From a clinical standpoint, predicting whether someone crosses a threshold is often more meaningful than predicting the exact number.

**2. Binary Logistic Regression — Quick Yes/No Screening**

- **Best for:** Identifying **high-risk individuals** (hsCRP > 3 mg/L). This makes it a useful tool for fast clinical screening.

- **Strength:** The AUC of **0.607** shows modest ability to separate high-risk from low-risk patients. Its simplicity makes it easy to apply in practice.

- **Limitation:** It collapses the “Low” and “Medium” groups into a single category, which removes valuable information. The model cannot distinguish what factors drive someone from Low to Medium risk.

**3. Generalized Ordinal Regression — Understanding the Full Risk Spectrum**

- **Best for:** Exploring how risk factors influence **different stages of inflammation** (Low → Medium → High).

- **Strength:** Unlike the simpler models, it uncovers **how the strength of effects changes across thresholds**.

Example: Physical activity (PAL) was protective at both thresholds, but much stronger in preventing **high inflammation** (OR = 0.59) compared to just preventing a shift from Low to Medium (OR = 0.74).

This level of detail is **hidden** in the other models. They can only say “exercise helps,” while this model tells us where it helps the most.

- **Limitation:** It is harder to explain and less accurate at predicting the extremes (Low = ~23% correct, High = ~29%) compared to the Medium group (~73%).

## **Final Recommendation**

- **Linear regression** is best when we want exact numerical predictions.

- **Binary logistic regression** is practical for quick, yes/no screening of high-risk cases.

- **Generalized ordinal logistic regression** is the most informative when the goal is to understand **how different risk factors shape the full progression of inflammation**.

In practice, the “right” model depends on whether the focus is on **precision, screening, or deeper understanding of risk pathways**.


# **Limitations**

**Variable Selection**
In real-world clinical research, choosing which variables to include in a model often involves **both statistical evidence and clinical judgment**. Some variables may be included even if their p-values are not statistically significant (e.g., up to 0.25) because of known clinical importance.

For this project, however, variable selection was based *strictly on statistical significance*. Only predictors that were significant in the regression analyses were included, and no additional clinical considerations were applied.

# References

